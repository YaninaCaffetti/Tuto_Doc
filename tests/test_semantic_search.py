# tests/test_semantic_search.py
"""
Mide la calidad de la b√∫squeda sem√°ntica realizada por los tutores expertos
del MoESystem contra un conjunto de validaci√≥n de par√°frasis y consultas trampa.

M√©tricas Clave Calculadas:
1.  **Precisi√≥n (Accuracy):** Porcentaje de prompts que mapean a la intenci√≥n correcta.
2.  **Cobertura:** Porcentaje de prompts que mapean a una intenci√≥n espec√≠fica (no 'default').
3.  **Estabilidad (Confianza):** Media y desviaci√≥n est√°ndar del score de confianza
    para las coincidencias correctas (excluyendo 'default').

Ejecuci√≥n:
    python tests/test_semantic_search.py

Salida:
    - Imprime las m√©tricas calculadas en la consola.
    - Genera un archivo CSV ('tests/validation_report.csv') con los resultados detallados por prompt.
    - Genera un archivo JSON ('tests/validation_metrics.json') con las m√©tricas agregadas.
"""

import sys
import os
import pandas as pd
import numpy as np
import json
import warnings # Importar warnings si no estaba ya

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

if project_root not in sys.path:
    print(f"(test_semantic_search.py) Adding project root to sys.path: {project_root}") # Mensaje de depuraci√≥n
    sys.path.append(project_root)


try:
    # Ahora esta importaci√≥n deber√≠a funcionar Y permitir la importaci√≥n anidada
    from src.cognitive_tutor import get_semantic_model, EXPERT_MAP, CUD_EXPERT #
    from sentence_transformers import util
    import torch
except ImportError as e:
    print(f"Error Cr√≠tico: No se pudieron importar los m√≥dulos necesarios.")
    print(f"Aseg√∫rate de que 'src/cognitive_tutor.py' y 'src/expert_kb.py' existan y sean correctos.")
    print(f"Detalle del error: {e}")
    sys.exit(1)
except KeyError as e:
    print(f"Error Cr√≠tico: No se encontr√≥ '{e}' esperado en 'cognitive_tutor.py'.")
    print("Verifica las definiciones de EXPERT_MAP y CUD_EXPERT.")
    sys.exit(1)

# --- 1. SET DE VALIDACI√ìN DE PAR√ÅFRASIS ---

validation_set = [
    # --- Pruebas para TutorCarrera ---
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "Mi curr√≠culum est√° desactualizado, ¬øqu√© le pongo?",
        "expected_intent_key": "¬øC√≥mo puedo mejorar mi CV?"
    },
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "No s√© cu√°nto deber√≠a ganar en mi pr√≥ximo trabajo.",
        "expected_intent_key": "¬øC√≥mo negociar mi salario?"
    },
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "Me siento estancado y no me valoran, creo que quiero renunciar.",
        "expected_intent_key": "¬øC√≥mo saber si debo cambiar de trabajo?"
    },

    # --- Pruebas para TutorInteraccion ---
    {
        "tutor_name": "TutorInteraccion",
        "prompt_usuario": "Siento que la gente no me entiende cuando hablo.",
        "expected_intent_key": "¬øQu√© hago si no me entienden cuando explico algo?"
    },
    {
        "tutor_name": "TutorInteraccion",
        "prompt_usuario": "Mi jefe me grit√≥ y no supe qu√© decir.",
        "expected_intent_key": "¬øC√≥mo manejar una cr√≠tica injusta?"
    },

    # --- Pruebas para TutorCompetencias ---
    {
        "tutor_name": "TutorCompetencias",
        "prompt_usuario": "Me bloqueo cuando intento aprender algo nuevo.",
        "expected_intent_key": "¬øC√≥mo superar el miedo a equivocarme cuando aprendo algo nuevo?"
    },
    {
        "tutor_name": "TutorCompetencias",
        "prompt_usuario": "Me distraigo mucho con el celular cuando quiero leer.",
        "expected_intent_key": "Me cuesta concentrarme cuando estudio"
    },

    # --- Pruebas para TutorBienestar ---
    {
        "tutor_name": "TutorBienestar",
        "prompt_usuario": "Estoy agotado todo el d√≠a.",
        "expected_intent_key": "Me siento muy cansado √∫ltimamente"
    },
    {
        "tutor_name": "TutorBienestar",
        "prompt_usuario": "No paro de pensar en todo lo que tengo que hacer y me paralizo.",
        "expected_intent_key": "Tengo miedo de no poder con todo"
    },

    # --- Pruebas para TutorApoyos ---
    {
        "tutor_name": "TutorApoyos",
        "prompt_usuario": "Si consigo un trabajo, ¬øme sacan la pensi√≥n?",
        "expected_intent_key": "Tengo miedo de perder mi pensi√≥n si empiezo a trabajar"
    },
    {
        "tutor_name": "TutorApoyos",
        "prompt_usuario": "El CUD me sirve para viajar gratis?",
        # Prueba dif√≠cil: la respuesta est√° *dentro* de la intenci√≥n, no es la clave exacta.
        "expected_intent_key": "¬øQu√© beneficios tengo con el Certificado √önico de Discapacidad?"
    },
    {
        "tutor_name": "TutorApoyos",
        "prompt_usuario": "Fui a la municipalidad y no tienen rampa.",
        "expected_intent_key": "Me enoja que las oficinas p√∫blicas no sean accesibles"
    },

    # --- Pruebas para TutorPrimerEmpleo ---
    {
        "tutor_name": "TutorPrimerEmpleo",
        "prompt_usuario": "Quiero trabajar pero nunca trabaj√©, ¬øqu√© hago?",
        "expected_intent_key": "No tengo experiencia laboral, ¬øc√≥mo puedo empezar?"
    },
    {
        "tutor_name": "TutorPrimerEmpleo",
        "prompt_usuario": "Me dijeron que me van a pagar en negro.",
        "expected_intent_key": "¬øQu√© hago si me piden trabajar sin registrarme?"
    },
    {
        "tutor_name": "TutorPrimerEmpleo",
        "prompt_usuario": "Me preguntaron por mi discapacidad en una entrevista y me sent√≠ mal.",
        "expected_intent_key": "¬øQu√© hago si me discriminan en una entrevista?"
    },

    # --- Pruebas para GestorCUD ---
    {
        "tutor_name": "GestorCUD",
        "prompt_usuario": "¬øQu√© es el CUD?",
        "expected_intent_key": "¬øQu√© es el Certificado √önico de Discapacidad?"
    },
    {
        "tutor_name": "GestorCUD",
        "prompt_usuario": "¬øSe paga para sacar el CUD?",
        "expected_intent_key": "¬øEl tr√°mite del CUD tiene costo?"
    },
    {
        "tutor_name": "GestorCUD",
        "prompt_usuario": "Me bocharon el CUD, ¬øqu√© hago ahora?",
        "expected_intent_key": "Me rechazaron el CUD, ¬øqu√© puedo hacer?"
    },


    # --- Prueba de "Consulta Trampa" (True Negative) ---
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "Tengo una charla la semana que viene y estoy nervioso.",
        # Intenci√≥n pertenece a TutorInteraccion ("Me da miedo presentar en p√∫blico").
        # TutorCarrera deber√≠a devolver 'default'.
        "expected_intent_key": "default"
    },
]

def run_validation():
    """
    Ejecuta el proceso de validaci√≥n sem√°ntica completo.

    Carga el modelo, itera sobre el `validation_set`, realiza la b√∫squeda
    sem√°ntica para cada prompt contra el tutor especificado, calcula las
    m√©tricas de Precisi√≥n, Cobertura y Estabilidad, y guarda los resultados.
    """
    print("--- üî¨ Iniciando Validaci√≥n del O√≠do Sem√°ntico (Tarea 1.2) ---")

    # Cargar el modelo sem√°ntico una sola vez
    model = get_semantic_model() #
    if not model:
        print("Error Cr√≠tico: No se pudo cargar el modelo sem√°ntico. Abortando validaci√≥n.")
        return

    # Unir todos los expertos (arquetipos + CUD) en un diccionario
    all_experts = EXPERT_MAP.copy() #
    all_experts["GestorCUD"] = CUD_EXPERT #

    results = [] # Lista para almacenar los resultados detallados

    # Pre-calcular embeddings para todos los expertos si a√∫n no lo est√°n
    print(f"‚Ä∫ Verificando/Calculando embeddings para {len(all_experts)} expertos...")
    for expert_name, expert_instance in all_experts.items():
        if expert_instance.kb_embeddings is None:
            print(f"  - Calculando para {expert_name}...")
            expert_instance._initialize_knowledge_base() #
    print("‚Ä∫ Embeddings listos.")

    # Iterar sobre cada caso de prueba en el set de validaci√≥n
    # --- üîß Mapa de alias entre nombres de tutores y arquetipos (compatibilidad para testeo) ---
    alias_map = {
        "TutorCarrera": "Prof_Subutil",
        "TutorInteraccion": "Com_Desafiado",
        "TutorCompetencias": "Nav_Informal",
        "TutorBienestar": "Potencial_Latente",
        "TutorApoyos": "Cand_Nec_Sig",
        "TutorPrimerEmpleo": "Joven_Transicion",
        "GestorCUD": "GestorCUD"
    }

    print(f"‚Ä∫ Ejecutando {len(validation_set)} pruebas de validaci√≥n...")
    for item in validation_set:
        tutor_name = item["tutor_name"]
        prompt = item["prompt_usuario"]
        expected_key = item["expected_intent_key"]

        # Usar alias si existe
        mapped_name = alias_map.get(tutor_name, tutor_name)
        tutor = all_experts.get(mapped_name)

        # --- Validaciones Previas ---
        if not tutor:
            warnings.warn(f"Saltando prueba: Tutor '{tutor_name}' no encontrado en all_experts.")
            continue
        # Verificar si el tutor tiene embeddings (puede ser None si la KB est√° vac√≠a o fall√≥ la codificaci√≥n)
        # O si kb_keys est√° vac√≠o (importante para evitar errores de √≠ndice)
        if tutor.kb_embeddings is None or not tutor.kb_keys:
            # Si se espera 'default', y no hay embeddings, es un "acierto" t√©cnico
            if expected_key == "default":
                 found_key = "default"
                 best_match_score = 0.0 # No hubo c√°lculo de score
                 is_correct = True
            else:
                 # Si se esperaba una clave espec√≠fica pero no hay embeddings, es un fallo
                 warnings.warn(f"Saltando prueba para {tutor_name}: KB vac√≠a o embeddings no calculados. Prompt: '{prompt}'")
                 found_key = "error_no_kb"
                 best_match_score = 0.0
                 is_correct = False

            results.append({
                "tutor": tutor_name, "prompt": prompt, "expected_key": expected_key,
                "found_key": found_key, "confidence": best_match_score,
                "is_correct": is_correct, "is_default": (found_key == "default")
            })
            continue # Pasar al siguiente item

        # --- Realizar la B√∫squeda Sem√°ntica ---
        try:
            prompt_embedding = model.encode(prompt, convert_to_tensor=True)
            # Calcular similitud coseno entre el prompt y todas las claves de la KB del tutor
            cos_scores = util.cos_sim(prompt_embedding, tutor.kb_embeddings)[0]
            # Encontrar el √≠ndice y score de la mejor coincidencia
            best_match_idx = torch.argmax(cos_scores).item()
            best_match_score = cos_scores[best_match_idx].item()

            found_key = "default" # Asumir default inicialmente
            # Aplicar el umbral espec√≠fico de CADA tutor
            if best_match_score > tutor.similarity_threshold: #
                # Si supera el umbral, obtener la 'pregunta_clave' correspondiente
                # Asegurarse de que el √≠ndice no est√© fuera de los l√≠mites
                if 0 <= best_match_idx < len(tutor.kb_keys):
                     found_key = tutor.kb_keys[best_match_idx] #
                else:
                     warnings.warn(f"√çndice fuera de rango ({best_match_idx}) para {tutor_name}. Usando default.")

            # Comparar la clave encontrada con la esperada
            is_correct = (found_key == expected_key)

            results.append({
                "tutor": tutor_name,
                "prompt": prompt,
                "expected_key": expected_key,
                "found_key": found_key,
                "confidence": best_match_score, # Guardar el score M√ÅXIMO encontrado
                "is_correct": is_correct,
                "is_default": (found_key == "default")
            })

        except Exception as search_error:
            warnings.warn(f"Error durante b√∫squeda sem√°ntica para {tutor_name} con prompt '{prompt}': {search_error}")
            results.append({
                "tutor": tutor_name, "prompt": prompt, "expected_key": expected_key,
                "found_key": "error_search", "confidence": 0.0,
                "is_correct": False, "is_default": False
            })


    if not results:
        print("Error Cr√≠tico: No se generaron resultados. Verifica el `validation_set` y los logs.")
        return

    # --- 2. CALCULAR M√âTRICAS AGREGADAS ---
    df = pd.DataFrame(results)

    # 1. Precisi√≥n (Accuracy): % de aciertos sobre el total de pruebas
    accuracy = df["is_correct"].mean() if not df.empty else 0.0

    # 2. Cobertura: % de respuestas que NO fueron 'default'
    coverage = 1.0 - df["is_default"].mean() if not df.empty else 0.0

    # 3. Estabilidad (Media y Std Dev de Confianza): Solo sobre aciertos que NO son 'default'
    correct_matches_df = df[(df["is_correct"] == True) & (df["is_default"] == False)]

    if len(correct_matches_df) > 1:
        confidence_mean = correct_matches_df["confidence"].mean()
        confidence_std = correct_matches_df["confidence"].std()
    elif len(correct_matches_df) == 1:
        # Si hay un solo acierto, la media es su confianza y la std dev es 0
        confidence_mean = correct_matches_df["confidence"].iloc[0]
        confidence_std = 0.0
    else:
        # Si no hay aciertos (correctos y no default), m√©tricas son 0
        confidence_mean = 0.0
        confidence_std = 0.0

    # --- 3. MOSTRAR RESULTADOS ---
    print("\n--- üìä Resultados de la Validaci√≥n ---")
    print(f"Total de Pruebas Ejecutadas: {len(df)}")

    # Imprimir m√©tricas con formato y comparaci√≥n con objetivos
    print(f"\n[M√©trica 1: Precisi√≥n] (Objetivo: >= 85%)")
    print(f"  ‚Ä∫ Semantic Intent Accuracy: {accuracy:.2%} {'‚úÖ' if accuracy >= 0.85 else '‚ùå'}")

    print(f"\n[M√©trica 2: Cobertura] (Objetivo: >= 90%)")
    print(f"  ‚Ä∫ Coverage (No-Default): {coverage:.2%} {'‚úÖ' if coverage >= 0.90 else '‚ùå'}")

    print(f"\n[M√©trica 3: Estabilidad] (Objetivo: <= 0.1)")
    print(f"  ‚Ä∫ Avg. Confidence (Correct Matches): {confidence_mean:.3f}")
    print(f"  ‚Ä∫ Std. Dev Confidence (Correct Matches): {confidence_std:.3f} {'‚úÖ' if confidence_std <= 0.1 else '‚ö†Ô∏è'}") # Advertencia si es alta

    # Opcional: Entrop√≠a (como se mencion√≥ en las notas doctorales)
    # (Implementaci√≥n m√°s avanzada, omitida por ahora para claridad)

    # --- 4. DETALLE DE FALLOS ---
    print("\n--- üßê Fallos Detallados ---")
    failures_df = df[df["is_correct"] == False]
    if failures_df.empty:
        print("‚úÖ ¬°Todas las pruebas pasaron!")
    else:
        print(f"({len(failures_df)} fallos detectados de {len(df)} pruebas)")
        # Imprimir detalles de cada fallo
        for _, row in failures_df.iterrows():
            print(f"  --- Fallo ---")
            print(f"    Tutor: {row['tutor']}")
            print(f"    Prompt: '{row['prompt']}'")
            print(f"    Esperado: '{row['expected_key']}'")
            print(f"    Obtenido: '{row['found_key']}' (Confianza M√°x: {row['confidence']:.3f})")

    # --- 5. GUARDAR REPORTES ---
    try:
        # Asegurarse de que el directorio 'tests' exista
        os.makedirs("tests", exist_ok=True)

        # Guardar reporte detallado en CSV
        report_path = os.path.join("tests", "validation_report.csv")
        df.to_csv(report_path, index=False, encoding='utf-8-sig') # Usar encoding para caracteres especiales
        print(f"\nReporte detallado guardado en: '{report_path}'")

        # Guardar m√©tricas agregadas en JSON
        metrics = {
            "accuracy": accuracy,
            "coverage": coverage,
            "confidence_mean_correct": confidence_mean,
            "confidence_std_correct": confidence_std,
            "total_tests": len(df),
            "total_failures": len(failures_df)
        }
        metrics_path = os.path.join("tests", "validation_metrics.json")
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=4)
        print(f"M√©tricas agregadas guardadas en: '{metrics_path}'")

    except Exception as e:
        warnings.warn(f"Error al guardar los reportes: {e}")

    print("\n--- Validaci√≥n Finalizada ---")

# Punto de entrada principal para ejecutar la validaci√≥n
if __name__ == "__main__":
    run_validation()
