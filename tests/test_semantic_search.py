# tests/test_semantic_search.py
"""
Script de ValidaciÃ³n

Mide la calidad de la bÃºsqueda semÃ¡ntica realizada por los tutores expertos
del MoESystem contra un conjunto de validaciÃ³n de parÃ¡frasis y consultas trampa.

MÃ©tricas Clave Calculadas:
1.  **PrecisiÃ³n (Accuracy):** Porcentaje de prompts que mapean a la intenciÃ³n correcta.
2.  **Cobertura:** Porcentaje de prompts que mapean a una intenciÃ³n especÃ­fica (no 'default').
3.  **Estabilidad (Confianza):** Media y desviaciÃ³n estÃ¡ndar del score de confianza
    para las coincidencias correctas (excluyendo 'default').

EjecuciÃ³n:
    python tests/test_semantic_search.py

Salida:
    - Imprime las mÃ©tricas calculadas en la consola.
    - Genera un archivo CSV ('tests/validation_report.csv') con los resultados detallados por prompt.
    - Genera un archivo JSON ('tests/validation_metrics.json') con las mÃ©tricas agregadas.
"""

import sys
import os
import pandas as pd
import numpy as np
import json
import warnings # Importar warnings si no estaba ya


project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

if project_root not in sys.path:
    print(f"(test_semantic_search.py) Adding project root to sys.path: {project_root}") # Mensaje de depuraciÃ³n
    sys.path.append(project_root)



try:
    # Ahora esta importaciÃ³n deberÃ­a funcionar Y permitir la importaciÃ³n anidada
    from src.cognitive_tutor import get_semantic_model, EXPERT_MAP, CUD_EXPERT #
    from sentence_transformers import util
    import torch
    import torch.nn.functional as F # *** NECESARIO PARA NORMALIZAR ***
except ImportError as e:
    print(f"Error CrÃ­tico: No se pudieron importar los mÃ³dulos necesarios.")
    print(f"AsegÃºrate de que 'src/cognitive_tutor.py' y 'src/expert_kb.py' existan y sean correctos.")
    print(f"Detalle del error: {e}")
    sys.exit(1)
except KeyError as e:
    print(f"Error CrÃ­tico: No se encontrÃ³ '{e}' esperado en 'cognitive_tutor.py'.")
    print("Verifica las definiciones de EXPERT_MAP y CUD_EXPERT.")
    sys.exit(1)

# --- 1. SET DE VALIDACIÃ“N DE PARÃFRASIS ---
# Define las consultas de prueba y la intenciÃ³n esperada para cada una.
# Incluye parÃ¡frasis (True Positives) y consultas trampa (True Negatives).
# *** ESTA LISTA HA SIDO ACTUALIZADA A LAS NUEVAS CLAVES FUNCIONALES ***

validation_set = [
    # --- Pruebas para TutorCarrera ---
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "Mi currÃ­culum estÃ¡ desactualizado, Â¿quÃ© le pongo?",
        "expected_intent_key": "OptimizaciÃ³n y mejora de un CV existente"
    },
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "No sÃ© cuÃ¡nto deberÃ­a ganar en mi prÃ³ximo trabajo.",
        "expected_intent_key": "Estrategias y tÃ©cnicas de negociaciÃ³n salarial" 
    },
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "Me siento estancado y no me valoran, creo que quiero renunciar.",
        "expected_intent_key": "EvaluaciÃ³n personal para decidir un cambio de empleo" 
    },

    # --- Pruebas para TutorInteraccion ---
    {
        "tutor_name": "TutorInteraccion",
        "prompt_usuario": "Siento que la gente no me entiende cuando hablo.",
        "expected_intent_key": "Mejora de la claridad didÃ¡ctica al explicar conceptos" 
    },
    {
        "tutor_name": "TutorInteraccion",
        "prompt_usuario": "Mi jefe me gritÃ³ y no supe quÃ© decir.",
        "expected_intent_key": "RecepciÃ³n asertiva de crÃ­ticas laborales (justas o injustas)" 
    },

    # --- Pruebas para TutorCompetencias ---
    {
        "tutor_name": "TutorCompetencias",
        "prompt_usuario": "Me bloqueo cuando intento aprender algo nuevo.",
        "expected_intent_key": "GestiÃ³n del miedo al error durante el aprendizaje" 
    },
    {
        "tutor_name": "TutorCompetencias",
        "prompt_usuario": "Me distraigo mucho con el celular cuando quiero leer.",
        "expected_intent_key": "TÃ©cnicas de concentraciÃ³n para el estudio (Pomodoro)" 
    },

    # --- Pruebas para TutorBienestar ---
    {
        "tutor_name": "TutorBienestar",
        "prompt_usuario": "Estoy agotado todo el dÃ­a.",
        "expected_intent_key": "Manejo de la fatiga, apatÃ­a y agotamiento emocional (burnout)" 
    },
    {
        "tutor_name": "TutorBienestar",
        "prompt_usuario": "No paro de pensar en todo lo que tengo que hacer y me paralizo.",
        "expected_intent_key": "Estrategias de gestiÃ³n de la ansiedad y el estrÃ©s por sobrecarga" 
    },

    # --- Pruebas para TutorApoyos ---
    {
        "tutor_name": "TutorApoyos",
        "prompt_usuario": "Si consigo un trabajo, Â¿me sacan la pensiÃ³n?",
        "expected_intent_key": "Compatibilidad entre pensiÃ³n por discapacidad y empleo formal" 
    },
    {
        "tutor_name": "TutorApoyos",
        "prompt_usuario": "El CUD me sirve para viajar gratis?",
        "expected_intent_key": "ReglamentaciÃ³n especÃ­fica del transporte gratuito para acompaÃ±ante con CUD." 
    },
    {
        "tutor_name": "TutorApoyos",
        "prompt_usuario": "Fui a la municipalidad y no tienen rampa.",
        "expected_intent_key": "GestiÃ³n de reclamos por falta de accesibilidad fÃ­sica (Ley 24.314)" 
    },

    # --- Pruebas para TutorPrimerEmpleo ---
    {
        "tutor_name": "TutorPrimerEmpleo",
        "prompt_usuario": "Quiero trabajar pero nunca trabajÃ©, Â¿quÃ© hago?",
        "expected_intent_key": "Estrategias de inserciÃ³n laboral sin experiencia previa (Programa JÃ³venes)" 
    },
    {
        "tutor_name": "TutorPrimerEmpleo",
        "prompt_usuario": "Me dijeron que me van a pagar en negro.",
        "expected_intent_key": "Acciones y denuncias contra el trabajo no registrado" 
    },
    {
        "tutor_name": "TutorPrimerEmpleo",
        "prompt_usuario": "Me preguntaron por mi discapacidad en una entrevista y me sentÃ­ mal.",
        "expected_intent_key": "Procedimiento legal por discriminaciÃ³n en entrevistas (Ley 23.592, ADAJUS)" 
    },

    # --- Pruebas para GestorCUD ---
    {
        "tutor_name": "GestorCUD",
        "prompt_usuario": "Â¿QuÃ© es el CUD?",
        "expected_intent_key": "ExplicaciÃ³n fundamental: QuÃ© es y para quÃ© sirve el CUD" 
    },
    {
        "tutor_name": "GestorCUD",
        "prompt_usuario": "Â¿Se paga para sacar el CUD?",
        "expected_intent_key": "ConfirmaciÃ³n de gratuidad y denuncia de cobros indebidos" 
    },
    {
        "tutor_name": "GestorCUD",
        "prompt_usuario": "Me bocharon el CUD, Â¿quÃ© hago ahora?",
        "expected_intent_key": "Procedimiento de apelaciÃ³n o revisiÃ³n ante rechazo del CUD" 
    },


    # --- Prueba de "Consulta Trampa" (True Negative) ---
    {
        "tutor_name": "TutorCarrera",
        "prompt_usuario": "Tengo una charla la semana que viene y estoy nervioso.",
        "expected_intent_key": "default" 
    },
]

def run_validation():
    """
    Ejecuta el proceso de validaciÃ³n semÃ¡ntica completo.

    Carga el modelo, itera sobre el `validation_set`, realiza la bÃºsqueda
    semÃ¡ntica para cada prompt contra el tutor especificado, calcula las
    mÃ©tricas de PrecisiÃ³n, Cobertura y Estabilidad, y guarda los resultados.
    """
    print("--- ğŸ”¬ Iniciando ValidaciÃ³n del OÃ­do SemÃ¡ntico")

    # Cargar el modelo semÃ¡ntico una sola vez
    model = get_semantic_model() #
    if not model:
        print("Error CrÃ­tico: No se pudo cargar el modelo semÃ¡ntico. Abortando validaciÃ³n.")
        return

    # Unir todos los expertos (arquetipos + CUD) en un diccionario
    all_experts = EXPERT_MAP.copy() #
    all_experts["GestorCUD"] = CUD_EXPERT #

    results = [] # Lista para almacenar los resultados detallados

    alias_map = {
        "TutorCarrera": "Prof_Subutil",
        "TutorInteraccion": "Com_Desafiado",
        "TutorCompetencias": "Nav_Informal",
        "TutorBienestar": "Potencial_Latente",
        "TutorApoyos": "Cand_Nec_Sig",
        "TutorPrimerEmpleo": "Joven_Transicion",
        "GestorCUD": "GestorCUD"
    }

    # Pre-calcular embeddings para todos los expertos si aÃºn no lo estÃ¡n
    print(f"â€º Forzando inicializaciÃ³n de embeddings para {len(all_experts)} expertos...")
    model_check = get_semantic_model() # Asegurar que el modelo estÃ© cargado
    if not model_check:
        print("Error CrÃ­tico: Modelo semÃ¡ntico no disponible para inicializar embeddings.")
        return
    
    # Usar el alias_map (definido arriba) para el log
    for expert_name, expert_instance in all_experts.items():
        # Mapeo inverso para log
        tutor_display_name = {v: k for k, v in alias_map.items()}.get(expert_name, expert_name)
        print(f"  - Inicializando {tutor_display_name}...")
        expert_instance._initialize_knowledge_base() 
    
    print("â€º Embeddings (re)inicializados.")


    print(f"â€º Ejecutando {len(validation_set)} pruebas de validaciÃ³n...")
    for item in validation_set:
        tutor_name = item["tutor_name"]
        prompt = item["prompt_usuario"]
        expected_key = item["expected_intent_key"]

        # Usar el alias si existe; si no, mantener el nombre original
        mapped_name = alias_map.get(tutor_name, tutor_name)
        tutor = all_experts.get(mapped_name)

        # --- Validaciones Previas ---
        if not tutor:
            warnings.warn(f"Saltando prueba: Tutor '{tutor_name}' (Mapeado a '{mapped_name}') no encontrado en all_experts.")
            continue
        if tutor.kb_embeddings is None or not tutor.kb_keys:
            if expected_key == "default":
                found_key = "default"
                best_match_score = 0.0
                is_correct = True
            else:
                warnings.warn(f"Saltando prueba para {tutor_name}: KB vacÃ­a o embeddings no calculados. Prompt: '{prompt}'")
                found_key = "error_no_kb"
                best_match_score = 0.0
                is_correct = False

            results.append({
                "tutor": tutor_name, "prompt": prompt, "expected_key": expected_key,
                "found_key": found_key, "confidence": best_match_score,
                "is_correct": is_correct, "is_default": (found_key == "default")
            })
            continue

        # --- Realizar la BÃºsqueda SemÃ¡ntica ---
        try:
            # 1. Codificar el prompt
            prompt_embedding_raw = model.encode(prompt, convert_to_tensor=True)
            prompt_embedding = F.normalize(prompt_embedding_raw, p=2, dim=0)

            device = prompt_embedding.device
            kb_embeds_device = tutor.kb_embeddings.to(device)
            
            cos_scores = util.cos_sim(prompt_embedding, kb_embeds_device)[0]
            best_match_idx = torch.argmax(cos_scores).item()
            best_match_score = cos_scores[best_match_idx].item()

            found_key = "default"
            if best_match_score > tutor.similarity_threshold: #
                if 0 <= best_match_idx < len(tutor.kb_keys):
                    found_key = tutor.kb_keys[best_match_idx] #
                else:
                    warnings.warn(f"Ãndice fuera de rango ({best_match_idx}) para {tutor_name}. Usando default.")

            is_correct = (found_key == expected_key)

            results.append({
                "tutor": tutor_name,
                "prompt": prompt,
                "expected_key": expected_key,
                "found_key": found_key,
                "confidence": best_match_score,
                "is_correct": is_correct,
                "is_default": (found_key == "default")
            })

        except Exception as search_error:
            warnings.warn(f"Error durante bÃºsqueda semÃ¡ntica para {tutor_name} con prompt '{prompt}': {search_error}")
            results.append({
                "tutor": tutor_name, "prompt": prompt, "expected_key": expected_key,
                "found_key": "error_search", "confidence": 0.0,
                "is_correct": False, "is_default": False
            })


    if not results:
        print("Error CrÃ­tico: No se generaron resultados. Verifica el `validation_set` y los logs.")
        return

    # --- 2. CALCULAR MÃ‰TRICAS AGREGADAS ---
    df = pd.DataFrame(results)
    accuracy = df["is_correct"].mean() if not df.empty else 0.0
    coverage = 1.0 - df["is_default"].mean() if not df.empty else 0.0
    correct_matches_df = df[(df["is_correct"] == True) & (df["is_default"] == False)]

    if len(correct_matches_df) > 1:
        confidence_mean = correct_matches_df["confidence"].mean()
        confidence_std = correct_matches_df["confidence"].std()
    elif len(correct_matches_df) == 1:
        confidence_mean = correct_matches_df["confidence"].iloc[0]
        confidence_std = 0.0
    else:
        confidence_mean = 0.0
        confidence_std = 0.0

    # --- 3. MOSTRAR RESULTADOS ---
    print("\n--- ğŸ“Š Resultados de la ValidaciÃ³n ---")
    print(f"Total de Pruebas Ejecutadas: {len(df)}")
    print(f"\n[MÃ©trica 1: PrecisiÃ³n] (Objetivo: >= 85%)")
    print(f"  â€º Semantic Intent Accuracy: {accuracy:.2%} {'âœ…' if accuracy >= 0.85 else 'âŒ'}")
    print(f"\n[MÃ©trica 2: Cobertura] (Objetivo: >= 90%)")
    print(f"  â€º Coverage (No-Default): {coverage:.2%} {'âœ…' if coverage >= 0.90 else 'âŒ'}")
    print(f"\n[MÃ©trica 3: Estabilidad] (Objetivo: <= 0.1)")
    print(f"  â€º Avg. Confidence (Correct Matches): {confidence_mean:.3f}")
    print(f"  â€º Std. Dev Confidence (Correct Matches): {confidence_std:.3f} {'âœ…' if confidence_std <= 0.1 else 'âš ï¸'}")

    # --- 4. DETALLE DE FALLOS ---
    print("\n--- ğŸ§ Fallos Detallados ---")
    failures_df = df[df["is_correct"] == False]
    if failures_df.empty:
        print("âœ… Â¡Todas las pruebas pasaron!")
    else:
        print(f"({len(failures_df)} fallos detectados de {len(df)} pruebas)")
        for _, row in failures_df.iterrows():
            print(f"  --- Fallo ---")
            print(f"    Tutor: {row['tutor']}")
            print(f"    Prompt: '{row['prompt']}'")
            print(f"    Esperado: '{row['expected_key']}'")
            print(f"    Obtenido: '{row['found_key']}' (Confianza MÃ¡x: {row['confidence']:.3f})")

    # --- 5. GUARDAR REPORTES ---
    try:
        os.makedirs("tests", exist_ok=True)
        report_path = os.path.join("tests", "validation_report.csv")
        df.to_csv(report_path, index=False, encoding='utf-8-sig')
        print(f"\nReporte detallado guardado en: '{report_path}'")
        metrics = {
            "accuracy": accuracy, "coverage": coverage,
            "confidence_mean_correct": confidence_mean, "confidence_std_correct": confidence_std,
            "total_tests": len(df), "total_failures": len(failures_df)
        }
        metrics_path = os.path.join("tests", "validation_metrics.json")
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=4)
        print(f"MÃ©tricas agregadas guardadas en: '{metrics_path}'")
    except Exception as e:
        warnings.warn(f"Error al guardar los reportes: {e}")

    print("\n--- ValidaciÃ³n Finalizada ---")

# Punto de entrada principal para ejecutar la validaciÃ³n
if __name__ == "__main__":
    run_validation()
